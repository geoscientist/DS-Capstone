---
title: "Capstone project"
author: "Aleksey Glotov"
date: "28 July 2016 Ð³."
output: html_document
---
## Load packages
```{r}
require(ggplot2)
require(tm)
require(RWeka)
```


## Data acquisition

```{r}
download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", destfile = "swiftkey.zip")
unzip(zipfile = "swiftkey.zip")

```

```{r}
con <- file("en_US/en_US.blogs.txt")
lines <- readLines(con)
lines <- sample(lines, length(lines)/5, replace = FALSE)
write(lines, "./sample/en_US.blogs.txt")
close(con)

con <- file("en_US/en_US.news.txt")
lines <- readLines(con)
lines <- sample(lines, length(lines)/5, replace = FALSE)
write(lines, "./sample/en_US.news.txt")
close(con)

con <- file("en_US/en_US.twitter.txt")
lines <- readLines(con)
lines <- sample(lines, length(lines)/5, replace = FALSE)
write(lines, "./sample/en_US.twitter.txt")
close(con)
```

Now we can create text collextion using 'tm' package and perform some data cleaning steps:

```{r, echo=FALSE}
#create text collection for 3 'en_US' files
textcol = Corpus(x = DirSource("sample"), readerControl = list(language = "en_US"))
```

```{r}
# preprocessing text collection
textcol <- tm_map(textcol, content_transformer(tolower))
textcol <- tm_map(textcol, content_transformer(removeNumbers))
textcol <- tm_map(textcol, content_transformer(removeWords), stopwords("english"))
# remove hash-tags
textcol <- tm_map(textcol, content_transformer(function(x) gsub("#[A-Za-z]", "", x)))
textcol <- tm_map(textcol, content_transformer(function(x) gsub('[\"/-:@]', "", x)))
# remove spaces in begin and end of each text
textcol <- tm_map(textcol, content_transformer(function(x) gsub("^ | $", "", x)))
textcol <- tm_map(textcol, content_transformer(stripWhitespace))
```

As we can see we have 3 different kinds of text data: blog, news and tweets. I suppose that relationship between words differs for words inside one sentence and words in other sentences (for example, we have last word in one sentence, and first word in next sentence). I think that predictive model must be trained on data splitted by sentences.
For splitting data we will use next symbols - '.', '!', '?', '...'.
After splitting we can remove punctuation symbols.

```{r}
textcol <- tm_map(textcol, content_transformer(function(x) gsub("[!?]", ".", x)))
textcol <- tm_map(textcol, content_transformer(function(x) gsub("\\.\\.\\.", ".", x)))
textcol <- tm_map(textcol, content_transformer(function(x) gsub("\\.\\.", ".", x)))
textcol <- tm_map(textcol, function(x) strsplit(x, "[.]", fixed = TRUE))
```

Now we need write function for tokenization text corpus
```{r}
Tokenizer <- function(x) NGramTokenizer(x, Weka_control(min=1, max=1))
BiTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=2, max=2))
TriTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=3, max=3))
```
Create tokenized documents
```{r}
dtm1 <- DocumentTermMatrix(textcol, control = list(tokenize = Tokenizer))

```
