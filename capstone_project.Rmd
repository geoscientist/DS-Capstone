---
title: "Capstone project"
author: "Aleksey Glotov"
date: "28 July 2016 Ð³."
output: html_document
---
## Load packages
```{r}
require(ggplot2)
require(tm)
require(RWeka)
```


## Data acquisition

```{r}
download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", destfile = "swiftkey.zip")
unzip(zipfile = "swiftkey.zip")

```

```{r}
con <- file("en_US/en_US.blogs.txt")
lines <- readLines(con)
lines <- sample(lines, length(lines)/5, replace = FALSE)
write(lines, "./sample/en_US.blogs.txt")
close(con)

con <- file("en_US/en_US.news.txt")
lines <- readLines(con)
lines <- sample(lines, length(lines)/5, replace = FALSE)
write(lines, "./sample/en_US.news.txt")
close(con)

con <- file("en_US/en_US.twitter.txt")
lines <- readLines(con)
lines <- sample(lines, length(lines)/5, replace = FALSE)
write(lines, "./sample/en_US.twitter.txt")
close(con)
```

Now we can create text collextion using 'tm' package and perform some data cleaning steps:

```{r, echo=FALSE}
#create text collection for 3 'en_US' files
textcol = Corpus(x = DirSource("sample"), readerControl = list(language = "en_US"))
# replace some punctuation symbols on dot
textcol <- tm_map(textcol, content_transformer(function(x) gsub("[!?]", ".", x)))
textcol <- tm_map(textcol, content_transformer(function(x) gsub("\\.\\.\\.", ".", x)))
textcol <- tm_map(textcol, content_transformer(function(x) gsub("\\.\\.", ".", x)))
# split all content by sentences
textcol <- tm_map(textcol, content_transformer(function(x) strsplit(x, "[.]")))
```

As we can see we have 3 different kinds of text data: blog, news and tweets. I suppose that relationship between words differs for words inside one sentence and words in other sentences (for example, we have last word in one sentence, and first word in next sentence). I think that predictive model must be trained on data splitted by sentences.
For splitting data we will use next symbols - '.', '!', '?', '...'.
After splitting we can remove punctuation symbols.

```{r}
# now we must transform content of text corpus in flat list, which we can analyze
text <- c(content(textcol[[1]]), content(textcol[[2]]), content(textcol[[3]]))
text <- strsplit(text, "[.]")
text <- unlist(text)
text <- temp[nchar(text) > 30]

```

```{r}
# preprocessing text
text <- tolower(text)
text <- removeNumbers(text)
text <- removeWords(text, stopwords("english"))
# remove hash-tags
text <- gsub("#[A-Za-z]", "", text)
text <- gsub('[\"/-:@)(]', "", text)
# remove spaces in begin and end of each text
text <- gsub("^ | $", "", text)
text <- removePunctuation(text)
text <- stripWhitespace(text)
```

Now we need write function for tokenization text corpus
```{r}
Tokenizer <- function(x) NGramTokenizer(x, Weka_control(min=1, max=1))
BiTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=2, max=2))
TriTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=3, max=3))
```
Create tokenized documents
```{r}
dtm1 <- DocumentTermMatrix(text, control = list(tokenize = Tokenizer))

```
